{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "directory_path_train = \"Data/train/\"\n",
    "\n",
    "corpus_train = []\n",
    "\n",
    "for filename in os.listdir(directory_path_train):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(directory_path_train, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            file_content = file.read()\n",
    "            try:\n",
    "                # Parse the string content as a dictionary\n",
    "                json_data = json.loads(file_content)\n",
    "                corpus_train.append(json_data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing JSON in file {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "directory_path_dev = \"Data/dev/\"\n",
    "\n",
    "corpus_dev = []\n",
    "\n",
    "for filename in os.listdir(directory_path_dev):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(directory_path_dev, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            file_content = file.read()\n",
    "            try:\n",
    "                # Parse the string content as a dictionary\n",
    "                json_data = json.loads(file_content)\n",
    "                corpus_dev.append(json_data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing JSON in file {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import *\n",
    "\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"tuner007/pegasus_paraphrase\")\n",
    "tokenizer = PegasusTokenizerFast.from_pretrained(\"tuner007/pegasus_paraphrase\")\n",
    "\n",
    "def get_paraphrased_sentences(model, tokenizer, sentence, num_return_sequences=5, num_beams=5):\n",
    "  # tokenize the text to be form of a list of token IDs\n",
    "  inputs = tokenizer([sentence], truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "  # generate the paraphrased sentences\n",
    "  outputs = model.generate(\n",
    "    **inputs,\n",
    "    num_beams=num_beams,\n",
    "    num_return_sequences=num_return_sequences,\n",
    "  )\n",
    "  # decode the generated sentences using the tokenizer to get them back to text\n",
    "  return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "#sentence = \"Learning is the process of acquiring new understanding, knowledge, behaviors, skills, values, attitudes, and preferences.\"\n",
    "#get_paraphrased_sentences(model, tokenizer, sentence, num_beams=2, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32my:\\UvA\\(DN) DL for NLP\\Project\\augmentations.ipynb Cell 5\u001b[0m line \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/y%3A/UvA/%28DN%29%20DL%20for%20NLP/Project/augmentations.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m corpus[\u001b[39m'\u001b[39m\u001b[39moptions\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m get_paraphrased_sentences(model, tokenizer, corpus[\u001b[39m'\u001b[39m\u001b[39moptions\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m], num_beams\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, num_return_sequences\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/y%3A/UvA/%28DN%29%20DL%20for%20NLP/Project/augmentations.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m corpus[\u001b[39m'\u001b[39m\u001b[39moptions\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m get_paraphrased_sentences(model, tokenizer, corpus[\u001b[39m'\u001b[39m\u001b[39moptions\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m1\u001b[39m], num_beams\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, num_return_sequences\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/y%3A/UvA/%28DN%29%20DL%20for%20NLP/Project/augmentations.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m corpus[\u001b[39m'\u001b[39m\u001b[39moptions\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m get_paraphrased_sentences(model, tokenizer, corpus[\u001b[39m'\u001b[39;49m\u001b[39moptions\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m2\u001b[39;49m], num_beams\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, num_return_sequences\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/y%3A/UvA/%28DN%29%20DL%20for%20NLP/Project/augmentations.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m corpus[\u001b[39m'\u001b[39m\u001b[39moptions\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m3\u001b[39m] \u001b[39m=\u001b[39m get_paraphrased_sentences(model, tokenizer, corpus[\u001b[39m'\u001b[39m\u001b[39moptions\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m3\u001b[39m], num_beams\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, num_return_sequences\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/y%3A/UvA/%28DN%29%20DL%20for%20NLP/Project/augmentations.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m corpus[\u001b[39m'\u001b[39m\u001b[39marticle\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m get_paraphrased_sentences(model, tokenizer, corpus[\u001b[39m'\u001b[39m\u001b[39marticle\u001b[39m\u001b[39m'\u001b[39m], num_beams\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, num_return_sequences\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;32my:\\UvA\\(DN) DL for NLP\\Project\\augmentations.ipynb Cell 5\u001b[0m line \u001b[0;36mget_paraphrased_sentences\u001b[1;34m(model, tokenizer, sentence, num_return_sequences, num_beams)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/y%3A/UvA/%28DN%29%20DL%20for%20NLP/Project/augmentations.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer([sentence], truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlongest\u001b[39m\u001b[39m\"\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/y%3A/UvA/%28DN%29%20DL%20for%20NLP/Project/augmentations.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# generate the paraphrased sentences\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/y%3A/UvA/%28DN%29%20DL%20for%20NLP/Project/augmentations.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(\n\u001b[0;32m     <a href='vscode-notebook-cell:/y%3A/UvA/%28DN%29%20DL%20for%20NLP/Project/augmentations.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m   \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs,\n\u001b[0;32m     <a href='vscode-notebook-cell:/y%3A/UvA/%28DN%29%20DL%20for%20NLP/Project/augmentations.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m   num_beams\u001b[39m=\u001b[39mnum_beams,\n\u001b[0;32m     <a href='vscode-notebook-cell:/y%3A/UvA/%28DN%29%20DL%20for%20NLP/Project/augmentations.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m   num_return_sequences\u001b[39m=\u001b[39mnum_return_sequences,\n\u001b[0;32m     <a href='vscode-notebook-cell:/y%3A/UvA/%28DN%29%20DL%20for%20NLP/Project/augmentations.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/y%3A/UvA/%28DN%29%20DL%20for%20NLP/Project/augmentations.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# decode the generated sentences using the tokenizer to get them back to text\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/y%3A/UvA/%28DN%29%20DL%20for%20NLP/Project/augmentations.ipynb#W4sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mbatch_decode(outputs, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\torch\\autograd\\grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     27\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[1;32m---> 28\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\transformers\\generation\\utils.py:1685\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1678\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1679\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1680\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   1681\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1682\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1683\u001b[0m     )\n\u001b[0;32m   1684\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[1;32m-> 1685\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeam_search(\n\u001b[0;32m   1686\u001b[0m         input_ids,\n\u001b[0;32m   1687\u001b[0m         beam_scorer,\n\u001b[0;32m   1688\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m   1689\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1690\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[0;32m   1691\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[0;32m   1692\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[0;32m   1693\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1694\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[0;32m   1695\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1696\u001b[0m     )\n\u001b[0;32m   1698\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SAMPLE:\n\u001b[0;32m   1699\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[0;32m   1700\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\transformers\\generation\\utils.py:3024\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   3020\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   3022\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m-> 3024\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[0;32m   3025\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[0;32m   3026\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   3027\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   3028\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   3029\u001b[0m )\n\u001b[0;32m   3031\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   3032\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:1415\u001b[0m, in \u001b[0;36mPegasusForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1410\u001b[0m     \u001b[39mif\u001b[39;00m decoder_input_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m decoder_inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1411\u001b[0m         decoder_input_ids \u001b[39m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   1412\u001b[0m             labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   1413\u001b[0m         )\n\u001b[1;32m-> 1415\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[0;32m   1416\u001b[0m     input_ids,\n\u001b[0;32m   1417\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1418\u001b[0m     decoder_input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[0;32m   1419\u001b[0m     encoder_outputs\u001b[39m=\u001b[39;49mencoder_outputs,\n\u001b[0;32m   1420\u001b[0m     decoder_attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[0;32m   1421\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1422\u001b[0m     decoder_head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[0;32m   1423\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[0;32m   1424\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1425\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1426\u001b[0m     decoder_inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[0;32m   1427\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1428\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1429\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1430\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1431\u001b[0m )\n\u001b[0;32m   1432\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(outputs[\u001b[39m0\u001b[39m]) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_logits_bias\n\u001b[0;32m   1434\u001b[0m masked_lm_loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:1277\u001b[0m, in \u001b[0;36mPegasusModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1271\u001b[0m         last_hidden_state\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m0\u001b[39m],\n\u001b[0;32m   1272\u001b[0m         hidden_states\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1273\u001b[0m         attentions\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1274\u001b[0m     )\n\u001b[0;32m   1276\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1277\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[0;32m   1278\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[0;32m   1279\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[0;32m   1280\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_outputs[\u001b[39m0\u001b[39;49m],\n\u001b[0;32m   1281\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1282\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[0;32m   1283\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[0;32m   1284\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1285\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[0;32m   1286\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1287\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1288\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1289\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1290\u001b[0m )\n\u001b[0;32m   1292\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1293\u001b[0m     \u001b[39mreturn\u001b[39;00m decoder_outputs \u001b[39m+\u001b[39m encoder_outputs\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:1109\u001b[0m, in \u001b[0;36mPegasusDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m   1099\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[0;32m   1100\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1106\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1107\u001b[0m     )\n\u001b[0;32m   1108\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1109\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[0;32m   1110\u001b[0m         hidden_states,\n\u001b[0;32m   1111\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1112\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1113\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m   1114\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49m(head_mask[idx] \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1115\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49m(\n\u001b[0;32m   1116\u001b[0m             cross_attn_head_mask[idx] \u001b[39mif\u001b[39;49;00m cross_attn_head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[0;32m   1117\u001b[0m         ),\n\u001b[0;32m   1118\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m   1119\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1120\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1121\u001b[0m     )\n\u001b[0;32m   1122\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1124\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:451\u001b[0m, in \u001b[0;36mPegasusDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39m# cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\u001b[39;00m\n\u001b[0;32m    450\u001b[0m cross_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m hidden_states, cross_attn_weights, cross_attn_present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder_attn(\n\u001b[0;32m    452\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[0;32m    453\u001b[0m     key_value_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    454\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m    455\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[0;32m    456\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mcross_attn_past_key_value,\n\u001b[0;32m    457\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    458\u001b[0m )\n\u001b[0;32m    459\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[0;32m    460\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:211\u001b[0m, in \u001b[0;36mPegasusAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[39melif\u001b[39;00m is_cross_attention:\n\u001b[0;32m    209\u001b[0m     \u001b[39m# cross_attentions\u001b[39;00m\n\u001b[0;32m    210\u001b[0m     key_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj(key_value_states), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, bsz)\n\u001b[1;32m--> 211\u001b[0m     value_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mv_proj(key_value_states), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, bsz)\n\u001b[0;32m    212\u001b[0m \u001b[39melif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[39m# reuse k, v, self_attention\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     key_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj(hidden_states), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, bsz)\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\torch\\nn\\functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[0;32m   1847\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[1;32m-> 1848\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import copy\n",
    "new_corpus = copy.deepcopy(corpus_dev)\n",
    "\n",
    "for corpus in new_corpus:\n",
    "    corpus['options'][0] = get_paraphrased_sentences(model, tokenizer, corpus['options'][0], num_beams=2, num_return_sequences=1)[0]\n",
    "    corpus['options'][1] = get_paraphrased_sentences(model, tokenizer, corpus['options'][1], num_beams=2, num_return_sequences=1)[0]\n",
    "    corpus['options'][2] = get_paraphrased_sentences(model, tokenizer, corpus['options'][2], num_beams=2, num_return_sequences=1)[0]\n",
    "    corpus['options'][3] = get_paraphrased_sentences(model, tokenizer, corpus['options'][3], num_beams=2, num_return_sequences=1)[0]\n",
    "    corpus['article'] = get_paraphrased_sentences(model, tokenizer, corpus['article'], num_beams=2, num_return_sequences=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': 'B',\n",
       " 'options': [\"f : i understand . so you 'll be at the office and get nothing to do at 4 o'clock . let 's have a talk at that time .\",\n",
       "  \"f : okay , so you 'll be at the office at 4 o'clock . maybe we can have a talk at 6 o'clock .\",\n",
       "  \"f : great ! see you at the airport at 4 o'clock .\",\n",
       "  \"f : no problem ! see you at the restaurant at 4 o'clock . i ca n't wait to eat dinner with you since we 've finished a long talk .\"],\n",
       " 'article': \"f : hi , victor . can we have a talk today ? m : i 'd love to , but i 've got a pretty tight schedule today . i 'll finish a report by 10:00 and then drive to the airport to pick up an engineer at 11:00. after that , i 'll have a meeting with him over lunch . i wo n't have a break until 2 o'clock . but then from 3:00 until 5:00 , i have to attend a senior staff meeting .\",\n",
       " 'id': 'dev_792'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_dev[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': 'B',\n",
       " 'options': [\"I understand that you will be at the office at 4 o'clock and not have anything to do.\",\n",
       "  \"We can have a talk at 6 o'clock if you are at the office at 4 o'clock.\",\n",
       "  \"At 4 o'clock, see you at the airport.\",\n",
       "  \"I'm going to eat dinner with you at 4 o'clock.\"],\n",
       " 'article': \"Can we have a talk today, victor? I'll finish my report by 10:00 and drive to the airport to pick up an engineer.\",\n",
       " 'id': 'dev_792'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Define the output directory where you want to save the .txt files\n",
    "output_directory = \"Data/dev_para/\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "for index, json_data in enumerate(corpus_dev):\n",
    "    # Create a unique filename for each item in the corpus (you can change this as needed)\n",
    "    filename = f\"recreated_{index}.txt\"\n",
    "    \n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(output_directory, filename)\n",
    "    \n",
    "    # Convert the JSON data back to a string\n",
    "    json_str = json.dumps(json_data, indent=4)  # You can adjust the indentation as needed\n",
    "    \n",
    "    # Write the JSON string to the file\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(json_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file source.spm from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-fr\\snapshots\\0b11e64c9efac19014daf61fb333354e09000f00\\source.spm\n",
      "loading file target.spm from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-fr\\snapshots\\0b11e64c9efac19014daf61fb333354e09000f00\\target.spm\n",
      "loading file vocab.json from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-fr\\snapshots\\0b11e64c9efac19014daf61fb333354e09000f00\\vocab.json\n",
      "loading file target_vocab.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-fr\\snapshots\\0b11e64c9efac19014daf61fb333354e09000f00\\tokenizer_config.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-fr\\snapshots\\0b11e64c9efac19014daf61fb333354e09000f00\\config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"Helsinki-NLP/opus-mt-en-fr\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      59513\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 59513,\n",
      "  \"decoder_vocab_size\": 59514,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 59513,\n",
      "  \"scale_embedding\": true,\n",
      "  \"share_encoder_decoder_embeddings\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.34.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 59514\n",
      "}\n",
      "\n",
      "Adding </s> to the vocabulary\n",
      "Adding <unk> to the vocabulary\n",
      "Adding <pad> to the vocabulary\n",
      "loading configuration file config.json from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-fr\\snapshots\\0b11e64c9efac19014daf61fb333354e09000f00\\config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/tmp/Helsinki-NLP/opus-mt-en-fr\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      59513\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 59513,\n",
      "  \"decoder_vocab_size\": 59514,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 59513,\n",
      "  \"scale_embedding\": true,\n",
      "  \"share_encoder_decoder_embeddings\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.34.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 59514\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-fr\\snapshots\\0b11e64c9efac19014daf61fb333354e09000f00\\pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      59513\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 59513,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"max_length\": 512,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 59513\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-fr.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-fr\\snapshots\\0b11e64c9efac19014daf61fb333354e09000f00\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      59513\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 59513,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"max_length\": 512,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 59513,\n",
      "  \"renormalize_logits\": true\n",
      "}\n",
      "\n",
      "loading file source.spm from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-fr-en\\snapshots\\b4a9a384c2ec68a224bbd2ee3fd5df0c71ca5b1b\\source.spm\n",
      "loading file target.spm from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-fr-en\\snapshots\\b4a9a384c2ec68a224bbd2ee3fd5df0c71ca5b1b\\target.spm\n",
      "loading file vocab.json from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-fr-en\\snapshots\\b4a9a384c2ec68a224bbd2ee3fd5df0c71ca5b1b\\vocab.json\n",
      "loading file target_vocab.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-fr-en\\snapshots\\b4a9a384c2ec68a224bbd2ee3fd5df0c71ca5b1b\\tokenizer_config.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-fr-en\\snapshots\\b4a9a384c2ec68a224bbd2ee3fd5df0c71ca5b1b\\config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"Helsinki-NLP/opus-mt-fr-en\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      59513\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 59513,\n",
      "  \"decoder_vocab_size\": 59514,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 59513,\n",
      "  \"scale_embedding\": true,\n",
      "  \"share_encoder_decoder_embeddings\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.34.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 59514\n",
      "}\n",
      "\n",
      "Adding </s> to the vocabulary\n",
      "Adding <unk> to the vocabulary\n",
      "Adding <pad> to the vocabulary\n",
      "loading configuration file config.json from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-fr-en\\snapshots\\b4a9a384c2ec68a224bbd2ee3fd5df0c71ca5b1b\\config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/tmp/Helsinki-NLP/opus-mt-fr-en\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      59513\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 59513,\n",
      "  \"decoder_vocab_size\": 59514,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 59513,\n",
      "  \"scale_embedding\": true,\n",
      "  \"share_encoder_decoder_embeddings\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.34.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 59514\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-fr-en\\snapshots\\b4a9a384c2ec68a224bbd2ee3fd5df0c71ca5b1b\\pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      59513\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 59513,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"max_length\": 512,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 59513\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-fr-en.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-fr-en\\snapshots\\b4a9a384c2ec68a224bbd2ee3fd5df0c71ca5b1b\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      59513\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 59513,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"max_length\": 512,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 59513,\n",
      "  \"renormalize_logits\": true\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Get the name of the first model\n",
    "first_model_name = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "\n",
    "# Get the tokenizer\n",
    "first_model_tkn = MarianTokenizer.from_pretrained(first_model_name)\n",
    "\n",
    "# Load the pretrained model based on the name\n",
    "first_model = MarianMTModel.from_pretrained(first_model_name)\n",
    "\n",
    "# Get the name of the second model\n",
    "second_model_name = 'Helsinki-NLP/opus-mt-fr-en'\n",
    "\n",
    "# Get the tokenizer\n",
    "second_model_tkn = MarianTokenizer.from_pretrained(second_model_name)\n",
    "\n",
    "# Load the pretrained model based on the name\n",
    "second_model = MarianMTModel.from_pretrained(second_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"f : hi , victor . can we have a talk today ? m : i 'd love to , but i 've got a pretty tight schedule today . i 'll finish a report by 10:00 and then drive to the airport to pick up an engineer at 11:00. after that , i 'll have a meeting with him over lunch . i wo n't have a break until 2 o'clock . but then from 3:00 until 5:00 , i have to attend a senior staff meeting .\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_texts = [corpus_dev[0]['article']]\n",
    "\n",
    "original_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"I'd like to, but I have a rather tight schedule today. I'll finish a report by 10:00 and then I'll drive to the airport to pick up an engineer at 11:00. After that, I'll have a meeting with him during lunch. I don't have a break until 2:00. But then from 3:00 to 5:00, I have to attend a senior staff meeting.\",\n",
       " \"f : hi , victor . can we have a talk today ? m : i 'd love to , but i 've got a pretty tight schedule today . i 'll finish a report by 10:00 and then drive to the airport to pick up an engineer at 11:00. after that , i 'll have a meeting with him over lunch . i wo n't have a break until 2 o'clock . but then from 3:00 until 5:00 , i have to attend a senior staff meeting .\"}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def format_batch_texts(language_code, batch_texts):\n",
    "  \n",
    "  formated_bach = [\">>{}<< {}\".format(language_code, text) for text in batch_texts]\n",
    "\n",
    "  return formated_bach\n",
    "\n",
    "def perform_translation(batch_texts, model, tokenizer, language=\"fr\"):\n",
    "  # Prepare the text data into appropriate format for the model\n",
    "  formated_batch_texts = format_batch_texts(language, batch_texts)\n",
    "  \n",
    "  # Generate translation using model\n",
    "  translated = model.generate(**tokenizer(formated_batch_texts, return_tensors=\"pt\", padding=True))\n",
    "\n",
    "  # Convert the generated tokens indices back into text\n",
    "  translated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "  \n",
    "  return translated_texts\n",
    "\n",
    "def perform_back_translation(batch_texts, original_language=\"en\", temporary_language=\"fr\"):\n",
    "\n",
    "  # Translate from Original to Temporary Language\n",
    "  tmp_translated_batch = perform_translation(batch_texts, first_model, first_model_tkn, temporary_language)\n",
    "\n",
    "  # Translate Back to English\n",
    "  back_translated_batch = perform_translation(tmp_translated_batch, second_model, second_model_tkn, original_language)\n",
    "\n",
    "  # Return The Final Result\n",
    "  return back_translated_batch\n",
    "\n",
    "def combine_texts(original_texts, back_translated_batch):\n",
    "  \n",
    "  return set(original_texts + back_translated_batch) \n",
    "\n",
    "def perform_back_translation_with_augmentation(batch_texts, original_language=\"en\", temporary_language=\"fr\"):\n",
    "\n",
    "  # Translate from Original to Temporary Language\n",
    "  tmp_translated_batch = perform_translation(batch_texts, first_model, first_model_tkn, temporary_language)\n",
    "\n",
    "  # Translate Back to English\n",
    "  back_translated_batch = perform_translation(tmp_translated_batch, second_model, second_model_tkn, original_language)\n",
    "\n",
    "  # Return The Final Result\n",
    "  return combine_texts(original_texts, back_translated_batch)\n",
    "\n",
    "\n",
    "\n",
    "final_augmented = perform_back_translation_with_augmentation(original_texts)\n",
    "final_augmented\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"f : hi , victor . can we have a talk today ? m : i 'd love to , but i 've got a pretty tight schedule today . i 'll finish a report by 10:00 and then drive to the airport to pick up an engineer at 11:00. after that , i 'll have a meeting with him over lunch . i wo n't have a break until 2 o'clock . but then from 3:00 until 5:00 , i have to attend a senior staff meeting .\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "new_corpus = copy.deepcopy(corpus_dev[0])\n",
    "\n",
    "new_corpus['article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I like to eat turtles and kill children with corpses of other childres.',\n",
       " ' I love turtles '}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perform_back_translation_with_augmentation({'I like turtles'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synonym Replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
