{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "directory_path_train = \"Data/train/\"\n",
    "\n",
    "corpus_train = []\n",
    "\n",
    "for filename in os.listdir(directory_path_train):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(directory_path_train, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            file_content = file.read()\n",
    "            try:\n",
    "                # Parse the string content as a dictionary\n",
    "                json_data = json.loads(file_content)\n",
    "                corpus_train.append(json_data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing JSON in file {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "directory_path_dev = \"Data/dev/\"\n",
    "\n",
    "corpus_dev = []\n",
    "\n",
    "for filename in os.listdir(directory_path_dev):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(directory_path_dev, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            file_content = file.read()\n",
    "            try:\n",
    "                # Parse the string content as a dictionary\n",
    "                json_data = json.loads(file_content)\n",
    "                corpus_dev.append(json_data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing JSON in file {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\transformers\\deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\torchaudio\\backend\\utils.py:67: UserWarning: No audio backend is available.\n",
      "  warnings.warn('No audio backend is available.')\n",
      "You are using torch==1.10.0, but torch>=1.12.0 is required to use TapasModel. Please upgrade torch.\n",
      "loading configuration file config.json from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--tuner007--pegasus_paraphrase\\snapshots\\0159e2949ca73657a2f1329898f51b7bb53b9ab2\\config.json\n",
      "Model config PegasusConfig {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"PegasusForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 16,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 16,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 1,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"max_position_embeddings\": 60,\n",
      "  \"model_type\": \"pegasus\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 8,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.34.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 96103\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--tuner007--pegasus_paraphrase\\snapshots\\0159e2949ca73657a2f1329898f51b7bb53b9ab2\\pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing PegasusForConditionalGeneration.\n",
      "\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at tuner007/pegasus_paraphrase and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "loading file spiece.model from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--tuner007--pegasus_paraphrase\\snapshots\\0159e2949ca73657a2f1329898f51b7bb53b9ab2\\spiece.model\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--tuner007--pegasus_paraphrase\\snapshots\\0159e2949ca73657a2f1329898f51b7bb53b9ab2\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--tuner007--pegasus_paraphrase\\snapshots\\0159e2949ca73657a2f1329898f51b7bb53b9ab2\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--tuner007--pegasus_paraphrase\\snapshots\\0159e2949ca73657a2f1329898f51b7bb53b9ab2\\config.json\n",
      "Model config PegasusConfig {\n",
      "  \"_name_or_path\": \"tuner007/pegasus_paraphrase\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"PegasusForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 16,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 16,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 1,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"max_position_embeddings\": 60,\n",
      "  \"model_type\": \"pegasus\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 8,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.34.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 96103\n",
      "}\n",
      "\n",
      "Adding </s> to the vocabulary\n",
      "Adding <unk> to the vocabulary\n",
      "Adding <pad> to the vocabulary\n",
      "Adding <mask_2> to the vocabulary\n",
      "Adding <mask_1> to the vocabulary\n",
      "Adding <unk_2> to the vocabulary\n",
      "Adding <unk_3> to the vocabulary\n",
      "Adding <unk_4> to the vocabulary\n",
      "Adding <unk_5> to the vocabulary\n",
      "Adding <unk_6> to the vocabulary\n",
      "Adding <unk_7> to the vocabulary\n",
      "Adding <unk_8> to the vocabulary\n",
      "Adding <unk_9> to the vocabulary\n",
      "Adding <unk_10> to the vocabulary\n",
      "Adding <unk_11> to the vocabulary\n",
      "Adding <unk_12> to the vocabulary\n",
      "Adding <unk_13> to the vocabulary\n",
      "Adding <unk_14> to the vocabulary\n",
      "Adding <unk_15> to the vocabulary\n",
      "Adding <unk_16> to the vocabulary\n",
      "Adding <unk_17> to the vocabulary\n",
      "Adding <unk_18> to the vocabulary\n",
      "Adding <unk_19> to the vocabulary\n",
      "Adding <unk_20> to the vocabulary\n",
      "Adding <unk_21> to the vocabulary\n",
      "Adding <unk_22> to the vocabulary\n",
      "Adding <unk_23> to the vocabulary\n",
      "Adding <unk_24> to the vocabulary\n",
      "Adding <unk_25> to the vocabulary\n",
      "Adding <unk_26> to the vocabulary\n",
      "Adding <unk_27> to the vocabulary\n",
      "Adding <unk_28> to the vocabulary\n",
      "Adding <unk_29> to the vocabulary\n",
      "Adding <unk_30> to the vocabulary\n",
      "Adding <unk_31> to the vocabulary\n",
      "Adding <unk_32> to the vocabulary\n",
      "Adding <unk_33> to the vocabulary\n",
      "Adding <unk_34> to the vocabulary\n",
      "Adding <unk_35> to the vocabulary\n",
      "Adding <unk_36> to the vocabulary\n",
      "Adding <unk_37> to the vocabulary\n",
      "Adding <unk_38> to the vocabulary\n",
      "Adding <unk_39> to the vocabulary\n",
      "Adding <unk_40> to the vocabulary\n",
      "Adding <unk_41> to the vocabulary\n",
      "Adding <unk_42> to the vocabulary\n",
      "Adding <unk_43> to the vocabulary\n",
      "Adding <unk_44> to the vocabulary\n",
      "Adding <unk_45> to the vocabulary\n",
      "Adding <unk_46> to the vocabulary\n",
      "Adding <unk_47> to the vocabulary\n",
      "Adding <unk_48> to the vocabulary\n",
      "Adding <unk_49> to the vocabulary\n",
      "Adding <unk_50> to the vocabulary\n",
      "Adding <unk_51> to the vocabulary\n",
      "Adding <unk_52> to the vocabulary\n",
      "Adding <unk_53> to the vocabulary\n",
      "Adding <unk_54> to the vocabulary\n",
      "Adding <unk_55> to the vocabulary\n",
      "Adding <unk_56> to the vocabulary\n",
      "Adding <unk_57> to the vocabulary\n",
      "Adding <unk_58> to the vocabulary\n",
      "Adding <unk_59> to the vocabulary\n",
      "Adding <unk_60> to the vocabulary\n",
      "Adding <unk_61> to the vocabulary\n",
      "Adding <unk_62> to the vocabulary\n",
      "Adding <unk_63> to the vocabulary\n",
      "Adding <unk_64> to the vocabulary\n",
      "Adding <unk_65> to the vocabulary\n",
      "Adding <unk_66> to the vocabulary\n",
      "Adding <unk_67> to the vocabulary\n",
      "Adding <unk_68> to the vocabulary\n",
      "Adding <unk_69> to the vocabulary\n",
      "Adding <unk_70> to the vocabulary\n",
      "Adding <unk_71> to the vocabulary\n",
      "Adding <unk_72> to the vocabulary\n",
      "Adding <unk_73> to the vocabulary\n",
      "Adding <unk_74> to the vocabulary\n",
      "Adding <unk_75> to the vocabulary\n",
      "Adding <unk_76> to the vocabulary\n",
      "Adding <unk_77> to the vocabulary\n",
      "Adding <unk_78> to the vocabulary\n",
      "Adding <unk_79> to the vocabulary\n",
      "Adding <unk_80> to the vocabulary\n",
      "Adding <unk_81> to the vocabulary\n",
      "Adding <unk_82> to the vocabulary\n",
      "Adding <unk_83> to the vocabulary\n",
      "Adding <unk_84> to the vocabulary\n",
      "Adding <unk_85> to the vocabulary\n",
      "Adding <unk_86> to the vocabulary\n",
      "Adding <unk_87> to the vocabulary\n",
      "Adding <unk_88> to the vocabulary\n",
      "Adding <unk_89> to the vocabulary\n",
      "Adding <unk_90> to the vocabulary\n",
      "Adding <unk_91> to the vocabulary\n",
      "Adding <unk_92> to the vocabulary\n",
      "Adding <unk_93> to the vocabulary\n",
      "Adding <unk_94> to the vocabulary\n",
      "Adding <unk_95> to the vocabulary\n",
      "Adding <unk_96> to the vocabulary\n",
      "Adding <unk_97> to the vocabulary\n",
      "Adding <unk_98> to the vocabulary\n",
      "Adding <unk_99> to the vocabulary\n",
      "Adding <unk_100> to the vocabulary\n",
      "Adding <unk_101> to the vocabulary\n",
      "Adding <unk_102> to the vocabulary\n",
      "loading configuration file config.json from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--tuner007--pegasus_paraphrase\\snapshots\\0159e2949ca73657a2f1329898f51b7bb53b9ab2\\config.json\n",
      "Model config PegasusConfig {\n",
      "  \"_name_or_path\": \"tuner007/pegasus_paraphrase\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"PegasusForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 16,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 16,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 1,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"max_position_embeddings\": 60,\n",
      "  \"model_type\": \"pegasus\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 8,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.34.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 96103\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import *\n",
    "\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"tuner007/pegasus_paraphrase\")\n",
    "tokenizer = PegasusTokenizerFast.from_pretrained(\"tuner007/pegasus_paraphrase\")\n",
    "\n",
    "def get_paraphrased_sentences(model, tokenizer, sentence, num_return_sequences=5, num_beams=5):\n",
    "  # tokenize the text to be form of a list of token IDs\n",
    "  inputs = tokenizer([sentence], truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "  # generate the paraphrased sentences\n",
    "  outputs = model.generate(\n",
    "    **inputs,\n",
    "    num_beams=num_beams,\n",
    "    num_return_sequences=num_return_sequences,\n",
    "  )\n",
    "  # decode the generated sentences using the tokenizer to get them back to text\n",
    "  return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "#sentence = \"Learning is the process of acquiring new understanding, knowledge, behaviors, skills, values, attitudes, and preferences.\"\n",
    "#get_paraphrased_sentences(model, tokenizer, sentence, num_beams=2, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "new_corpus = copy.deepcopy(corpus_dev)\n",
    "\n",
    "for corpus in new_corpus:\n",
    "    corpus['options'][0] = get_paraphrased_sentences(model, tokenizer, corpus['options'][0], num_beams=2, num_return_sequences=1)[0]\n",
    "    corpus['options'][1] = get_paraphrased_sentences(model, tokenizer, corpus['options'][1], num_beams=2, num_return_sequences=1)[0]\n",
    "    corpus['options'][2] = get_paraphrased_sentences(model, tokenizer, corpus['options'][2], num_beams=2, num_return_sequences=1)[0]\n",
    "    corpus['options'][3] = get_paraphrased_sentences(model, tokenizer, corpus['options'][3], num_beams=2, num_return_sequences=1)[0]\n",
    "    corpus['article'] = get_paraphrased_sentences(model, tokenizer, corpus['article'], num_beams=2, num_return_sequences=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "corpus = copy.deepcopy(corpus_dev)\n",
    "\n",
    "corpus[0]['options'][0] = get_paraphrased_sentences(model, tokenizer, corpus[0]['options'][0], num_beams=2, num_return_sequences=1)[0]\n",
    "corpus[0]['options'][1] = get_paraphrased_sentences(model, tokenizer, corpus[0]['options'][1], num_beams=2, num_return_sequences=1)[0]\n",
    "corpus[0]['options'][2] = get_paraphrased_sentences(model, tokenizer, corpus[0]['options'][2], num_beams=2, num_return_sequences=1)[0]\n",
    "corpus[0]['options'][3] = get_paraphrased_sentences(model, tokenizer, corpus[0]['options'][3], num_beams=2, num_return_sequences=1)[0]\n",
    "corpus[0]['article'] = get_paraphrased_sentences(model, tokenizer, corpus[0]['article'], num_beams=2, num_return_sequences=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': 'B',\n",
       " 'options': [\"f : i understand . so you 'll be at the office and get nothing to do at 4 o'clock . let 's have a talk at that time .\",\n",
       "  \"f : okay , so you 'll be at the office at 4 o'clock . maybe we can have a talk at 6 o'clock .\",\n",
       "  \"f : great ! see you at the airport at 4 o'clock .\",\n",
       "  \"f : no problem ! see you at the restaurant at 4 o'clock . i ca n't wait to eat dinner with you since we 've finished a long talk .\"],\n",
       " 'article': \"f : hi , victor . can we have a talk today ? m : i 'd love to , but i 've got a pretty tight schedule today . i 'll finish a report by 10:00 and then drive to the airport to pick up an engineer at 11:00. after that , i 'll have a meeting with him over lunch . i wo n't have a break until 2 o'clock . but then from 3:00 until 5:00 , i have to attend a senior staff meeting .\",\n",
       " 'id': 'dev_792'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_dev[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': 'B',\n",
       " 'options': [\"I understand that you will be at the office at 4 o'clock and not have anything to do.\",\n",
       "  \"We can have a talk at 6 o'clock if you are at the office at 4 o'clock.\",\n",
       "  \"At 4 o'clock, see you at the airport.\",\n",
       "  \"I'm going to eat dinner with you at 4 o'clock.\"],\n",
       " 'article': \"Can we have a talk today, victor? I'll finish my report by 10:00 and drive to the airport to pick up an engineer.\",\n",
       " 'id': 'dev_792'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Define the output directory where you want to save the .txt files\n",
    "output_directory = \"Data/dev_para/\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "for index, json_data in enumerate(corpus):\n",
    "#for index, json_data in enumerate(new_corpus):\n",
    "    # Create a unique filename for each item in the corpus (you can change this as needed)\n",
    "    filename = f\"augmented_{index}.txt\"\n",
    "    \n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(output_directory, filename)\n",
    "    \n",
    "    # Convert the JSON data back to a string\n",
    "    json_str = json.dumps(json_data, indent=4)  # You can adjust the indentation as needed\n",
    "    \n",
    "    # Write the JSON string to the file\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(json_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Get the name of the first model\n",
    "first_model_name = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "\n",
    "# Get the tokenizer\n",
    "first_model_tkn = MarianTokenizer.from_pretrained(first_model_name)\n",
    "\n",
    "# Load the pretrained model based on the name\n",
    "first_model = MarianMTModel.from_pretrained(first_model_name)\n",
    "\n",
    "# Get the name of the second model\n",
    "second_model_name = 'Helsinki-NLP/opus-mt-fr-en'\n",
    "\n",
    "# Get the tokenizer\n",
    "second_model_tkn = MarianTokenizer.from_pretrained(second_model_name)\n",
    "\n",
    "# Load the pretrained model based on the name\n",
    "second_model = MarianMTModel.from_pretrained(second_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_batch_texts(language_code, batch_texts):\n",
    "  \n",
    "  formated_bach = [\">>{}<< {}\".format(language_code, text) for text in batch_texts]\n",
    "\n",
    "  return formated_bach\n",
    "\n",
    "def perform_translation(batch_texts, model, tokenizer, language=\"fr\"):\n",
    "  # Prepare the text data into appropriate format for the model\n",
    "  formated_batch_texts = format_batch_texts(language, batch_texts)\n",
    "  \n",
    "  # Generate translation using model\n",
    "  translated = model.generate(**tokenizer(formated_batch_texts, return_tensors=\"pt\", padding=True))\n",
    "\n",
    "  # Convert the generated tokens indices back into text\n",
    "  translated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "  \n",
    "  return translated_texts\n",
    "\n",
    "def perform_back_translation(batch_texts, original_language=\"en\", temporary_language=\"fr\"):\n",
    "\n",
    "  # Translate from Original to Tempora ry Language\n",
    "  tmp_translated_batch = perform_translation(batch_texts, first_model, first_model_tkn, temporary_language)\n",
    "\n",
    "  # Translate Back to English\n",
    "  back_translated_batch = perform_translation(tmp_translated_batch, second_model, second_model_tkn, original_language)\n",
    "\n",
    "  # Return The Final Result\n",
    "  return back_translated_batch\n",
    "\n",
    "def combine_texts(original_texts, back_translated_batch):\n",
    "  \n",
    "  return set(original_texts + back_translated_batch) \n",
    "\n",
    "def perform_back_translation_with_augmentation(batch_texts, original_language=\"en\", temporary_language=\"fr\"):\n",
    "\n",
    "  # Translate from Original to Temporary Language\n",
    "  tmp_translated_batch = perform_translation(batch_texts, first_model, first_model_tkn, temporary_language)\n",
    "\n",
    "  # Translate Back to English\n",
    "  back_translated_batch = perform_translation(tmp_translated_batch, second_model, second_model_tkn, original_language)\n",
    "\n",
    "  # Return The Final Result\n",
    "  # return combine_texts(original_texts, back_translated_batch)\n",
    "  return back_translated_batch\n",
    "\n",
    "\n",
    "\n",
    "#final_augmented = perform_back_translation_with_augmentation(original_texts)\n",
    "#final_augmented\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"m : have you seen my gloves anywhere ? i 've checked the cupboard but they 're not there . did i leave them on the desk ? f : oh yes , i remember i moved them from there and put them on the shelf by the window . i needed to do some work on the desk .\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "new_corpus = copy.deepcopy(corpus_dev[2])\n",
    "\n",
    "new_corpus['article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"F: I understand, so you'll be at the office and have nothing to do at 4 o'clock. Let's have a conversation at that time.\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perform_back_translation_with_augmentation({'I like turtles'})\n",
    "\n",
    "perform_back_translation_with_augmentation({corpus[0]['options'][0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "corpus = copy.deepcopy(corpus_dev)\n",
    "\n",
    "corpus[0]['options'][0] = perform_back_translation_with_augmentation({corpus[0]['options'][0]})\n",
    "corpus[0]['options'][1] = perform_back_translation_with_augmentation({corpus[0]['options'][1]})\n",
    "corpus[0]['options'][2] = perform_back_translation_with_augmentation({corpus[0]['options'][2]})\n",
    "corpus[0]['options'][3] = perform_back_translation_with_augmentation({corpus[0]['options'][3]})\n",
    "corpus[0]['article'] = perform_back_translation_with_augmentation({corpus[0]['article']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "new_corpus = copy.deepcopy(corpus_dev)\n",
    "\n",
    "\n",
    "for corpus in new_corpus:\n",
    "    corpus['options'][0] = perform_back_translation_with_augmentation({corpus['options'][0]})\n",
    "    corpus['options'][1] = perform_back_translation_with_augmentation({corpus['options'][1]})\n",
    "    corpus['options'][2] = perform_back_translation_with_augmentation({corpus['options'][2]})\n",
    "    corpus['options'][3] = perform_back_translation_with_augmentation({corpus['options'][3]})\n",
    "    corpus['article'] = perform_back_translation_with_augmentation({corpus['article']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': 'B',\n",
       " 'options': [\"f : i understand . so you 'll be at the office and get nothing to do at 4 o'clock . let 's have a talk at that time .\",\n",
       "  \"f : okay , so you 'll be at the office at 4 o'clock . maybe we can have a talk at 6 o'clock .\",\n",
       "  \"f : great ! see you at the airport at 4 o'clock .\",\n",
       "  \"f : no problem ! see you at the restaurant at 4 o'clock . i ca n't wait to eat dinner with you since we 've finished a long talk .\"],\n",
       " 'article': \"f : hi , victor . can we have a talk today ? m : i 'd love to , but i 've got a pretty tight schedule today . i 'll finish a report by 10:00 and then drive to the airport to pick up an engineer at 11:00. after that , i 'll have a meeting with him over lunch . i wo n't have a break until 2 o'clock . but then from 3:00 until 5:00 , i have to attend a senior staff meeting .\",\n",
       " 'id': 'dev_792'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_dev[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': 'B',\n",
       " 'options': [[\"F: I understand, so you'll be at the office and have nothing to do at 4 o'clock. Let's have a conversation at that time.\"],\n",
       "  [\"Okay, so you'll be in the office at 4 o'clock, maybe we can talk at 6 o'clock.\"],\n",
       "  [\"F: Great! Meet at the airport at 4 o'clock.\"],\n",
       "  [\"F: No problem! Go to the restaurant at 4 o'clock. I'm not looking forward to dinner with you since we finished a long conversation.\"]],\n",
       " 'article': [\"I'd like to, but I have a rather tight schedule today. I'll finish a report by 10:00 and then I'll drive to the airport to pick up an engineer at 11:00. After that, I'll have a meeting with him during lunch. I don't have a break until 2:00. But then from 3:00 to 5:00, I have to attend a senior staff meeting.\"],\n",
       " 'id': 'dev_792'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synonym Replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
