{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "directory_path_train = \"Data/train/\"\n",
    "\n",
    "corpus_train = []\n",
    "\n",
    "for filename in os.listdir(directory_path_train):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(directory_path_train, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            file_content = file.read()\n",
    "            try:\n",
    "                # Parse the string content as a dictionary\n",
    "                json_data = json.loads(file_content)\n",
    "                corpus_train.append(json_data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing JSON in file {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "directory_path_dev = \"Data/dev/\"\n",
    "\n",
    "corpus_dev = []\n",
    "\n",
    "for filename in os.listdir(directory_path_dev):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(directory_path_dev, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            file_content = file.read()\n",
    "            try:\n",
    "                # Parse the string content as a dictionary\n",
    "                json_data = json.loads(file_content)\n",
    "                corpus_dev.append(json_data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing JSON in file {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\transformers\\deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\torchaudio\\backend\\utils.py:67: UserWarning: No audio backend is available.\n",
      "  warnings.warn('No audio backend is available.')\n",
      "You are using torch==1.10.0, but torch>=1.12.0 is required to use TapasModel. Please upgrade torch.\n",
      "loading configuration file config.json from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--tuner007--pegasus_paraphrase\\snapshots\\0159e2949ca73657a2f1329898f51b7bb53b9ab2\\config.json\n",
      "Model config PegasusConfig {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"PegasusForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 16,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 16,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 1,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"max_position_embeddings\": 60,\n",
      "  \"model_type\": \"pegasus\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 8,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.34.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 96103\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--tuner007--pegasus_paraphrase\\snapshots\\0159e2949ca73657a2f1329898f51b7bb53b9ab2\\pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing PegasusForConditionalGeneration.\n",
      "\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at tuner007/pegasus_paraphrase and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "loading file spiece.model from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--tuner007--pegasus_paraphrase\\snapshots\\0159e2949ca73657a2f1329898f51b7bb53b9ab2\\spiece.model\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--tuner007--pegasus_paraphrase\\snapshots\\0159e2949ca73657a2f1329898f51b7bb53b9ab2\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--tuner007--pegasus_paraphrase\\snapshots\\0159e2949ca73657a2f1329898f51b7bb53b9ab2\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--tuner007--pegasus_paraphrase\\snapshots\\0159e2949ca73657a2f1329898f51b7bb53b9ab2\\config.json\n",
      "Model config PegasusConfig {\n",
      "  \"_name_or_path\": \"tuner007/pegasus_paraphrase\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"PegasusForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 16,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 16,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 1,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"max_position_embeddings\": 60,\n",
      "  \"model_type\": \"pegasus\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 8,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.34.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 96103\n",
      "}\n",
      "\n",
      "Adding </s> to the vocabulary\n",
      "Adding <unk> to the vocabulary\n",
      "Adding <pad> to the vocabulary\n",
      "Adding <mask_2> to the vocabulary\n",
      "Adding <mask_1> to the vocabulary\n",
      "Adding <unk_2> to the vocabulary\n",
      "Adding <unk_3> to the vocabulary\n",
      "Adding <unk_4> to the vocabulary\n",
      "Adding <unk_5> to the vocabulary\n",
      "Adding <unk_6> to the vocabulary\n",
      "Adding <unk_7> to the vocabulary\n",
      "Adding <unk_8> to the vocabulary\n",
      "Adding <unk_9> to the vocabulary\n",
      "Adding <unk_10> to the vocabulary\n",
      "Adding <unk_11> to the vocabulary\n",
      "Adding <unk_12> to the vocabulary\n",
      "Adding <unk_13> to the vocabulary\n",
      "Adding <unk_14> to the vocabulary\n",
      "Adding <unk_15> to the vocabulary\n",
      "Adding <unk_16> to the vocabulary\n",
      "Adding <unk_17> to the vocabulary\n",
      "Adding <unk_18> to the vocabulary\n",
      "Adding <unk_19> to the vocabulary\n",
      "Adding <unk_20> to the vocabulary\n",
      "Adding <unk_21> to the vocabulary\n",
      "Adding <unk_22> to the vocabulary\n",
      "Adding <unk_23> to the vocabulary\n",
      "Adding <unk_24> to the vocabulary\n",
      "Adding <unk_25> to the vocabulary\n",
      "Adding <unk_26> to the vocabulary\n",
      "Adding <unk_27> to the vocabulary\n",
      "Adding <unk_28> to the vocabulary\n",
      "Adding <unk_29> to the vocabulary\n",
      "Adding <unk_30> to the vocabulary\n",
      "Adding <unk_31> to the vocabulary\n",
      "Adding <unk_32> to the vocabulary\n",
      "Adding <unk_33> to the vocabulary\n",
      "Adding <unk_34> to the vocabulary\n",
      "Adding <unk_35> to the vocabulary\n",
      "Adding <unk_36> to the vocabulary\n",
      "Adding <unk_37> to the vocabulary\n",
      "Adding <unk_38> to the vocabulary\n",
      "Adding <unk_39> to the vocabulary\n",
      "Adding <unk_40> to the vocabulary\n",
      "Adding <unk_41> to the vocabulary\n",
      "Adding <unk_42> to the vocabulary\n",
      "Adding <unk_43> to the vocabulary\n",
      "Adding <unk_44> to the vocabulary\n",
      "Adding <unk_45> to the vocabulary\n",
      "Adding <unk_46> to the vocabulary\n",
      "Adding <unk_47> to the vocabulary\n",
      "Adding <unk_48> to the vocabulary\n",
      "Adding <unk_49> to the vocabulary\n",
      "Adding <unk_50> to the vocabulary\n",
      "Adding <unk_51> to the vocabulary\n",
      "Adding <unk_52> to the vocabulary\n",
      "Adding <unk_53> to the vocabulary\n",
      "Adding <unk_54> to the vocabulary\n",
      "Adding <unk_55> to the vocabulary\n",
      "Adding <unk_56> to the vocabulary\n",
      "Adding <unk_57> to the vocabulary\n",
      "Adding <unk_58> to the vocabulary\n",
      "Adding <unk_59> to the vocabulary\n",
      "Adding <unk_60> to the vocabulary\n",
      "Adding <unk_61> to the vocabulary\n",
      "Adding <unk_62> to the vocabulary\n",
      "Adding <unk_63> to the vocabulary\n",
      "Adding <unk_64> to the vocabulary\n",
      "Adding <unk_65> to the vocabulary\n",
      "Adding <unk_66> to the vocabulary\n",
      "Adding <unk_67> to the vocabulary\n",
      "Adding <unk_68> to the vocabulary\n",
      "Adding <unk_69> to the vocabulary\n",
      "Adding <unk_70> to the vocabulary\n",
      "Adding <unk_71> to the vocabulary\n",
      "Adding <unk_72> to the vocabulary\n",
      "Adding <unk_73> to the vocabulary\n",
      "Adding <unk_74> to the vocabulary\n",
      "Adding <unk_75> to the vocabulary\n",
      "Adding <unk_76> to the vocabulary\n",
      "Adding <unk_77> to the vocabulary\n",
      "Adding <unk_78> to the vocabulary\n",
      "Adding <unk_79> to the vocabulary\n",
      "Adding <unk_80> to the vocabulary\n",
      "Adding <unk_81> to the vocabulary\n",
      "Adding <unk_82> to the vocabulary\n",
      "Adding <unk_83> to the vocabulary\n",
      "Adding <unk_84> to the vocabulary\n",
      "Adding <unk_85> to the vocabulary\n",
      "Adding <unk_86> to the vocabulary\n",
      "Adding <unk_87> to the vocabulary\n",
      "Adding <unk_88> to the vocabulary\n",
      "Adding <unk_89> to the vocabulary\n",
      "Adding <unk_90> to the vocabulary\n",
      "Adding <unk_91> to the vocabulary\n",
      "Adding <unk_92> to the vocabulary\n",
      "Adding <unk_93> to the vocabulary\n",
      "Adding <unk_94> to the vocabulary\n",
      "Adding <unk_95> to the vocabulary\n",
      "Adding <unk_96> to the vocabulary\n",
      "Adding <unk_97> to the vocabulary\n",
      "Adding <unk_98> to the vocabulary\n",
      "Adding <unk_99> to the vocabulary\n",
      "Adding <unk_100> to the vocabulary\n",
      "Adding <unk_101> to the vocabulary\n",
      "Adding <unk_102> to the vocabulary\n",
      "loading configuration file config.json from cache at C:\\Users\\Bunplaya/.cache\\huggingface\\hub\\models--tuner007--pegasus_paraphrase\\snapshots\\0159e2949ca73657a2f1329898f51b7bb53b9ab2\\config.json\n",
      "Model config PegasusConfig {\n",
      "  \"_name_or_path\": \"tuner007/pegasus_paraphrase\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"PegasusForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 16,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 16,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 1,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"max_position_embeddings\": 60,\n",
      "  \"model_type\": \"pegasus\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 8,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.34.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 96103\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import *\n",
    "\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"tuner007/pegasus_paraphrase\")\n",
    "tokenizer = PegasusTokenizerFast.from_pretrained(\"tuner007/pegasus_paraphrase\")\n",
    "\n",
    "def get_paraphrased_sentences(model, tokenizer, sentence, num_return_sequences=5, num_beams=5):\n",
    "  # tokenize the text to be form of a list of token IDs\n",
    "  inputs = tokenizer([sentence], truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "  # generate the paraphrased sentences\n",
    "  outputs = model.generate(\n",
    "    **inputs,\n",
    "    num_beams=num_beams,\n",
    "    num_return_sequences=num_return_sequences,\n",
    "  )\n",
    "  # decode the generated sentences using the tokenizer to get them back to text\n",
    "  return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "#sentence = \"Learning is the process of acquiring new understanding, knowledge, behaviors, skills, values, attitudes, and preferences.\"\n",
    "#get_paraphrased_sentences(model, tokenizer, sentence, num_beams=2, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "new_corpus = copy.deepcopy(corpus_dev)\n",
    "\n",
    "for corpus in new_corpus:\n",
    "    corpus['options'][0] = get_paraphrased_sentences(model, tokenizer, corpus['options'][0], num_beams=2, num_return_sequences=1)[0]\n",
    "    corpus['options'][1] = get_paraphrased_sentences(model, tokenizer, corpus['options'][1], num_beams=2, num_return_sequences=1)[0]\n",
    "    corpus['options'][2] = get_paraphrased_sentences(model, tokenizer, corpus['options'][2], num_beams=2, num_return_sequences=1)[0]\n",
    "    corpus['options'][3] = get_paraphrased_sentences(model, tokenizer, corpus['options'][3], num_beams=2, num_return_sequences=1)[0]\n",
    "    corpus['article'] = get_paraphrased_sentences(model, tokenizer, corpus['article'], num_beams=2, num_return_sequences=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "corpus = copy.deepcopy(corpus_dev)\n",
    "\n",
    "corpus[0]['options'][0] = get_paraphrased_sentences(model, tokenizer, corpus[0]['options'][0], num_beams=2, num_return_sequences=1)[0]\n",
    "corpus[0]['options'][1] = get_paraphrased_sentences(model, tokenizer, corpus[0]['options'][1], num_beams=2, num_return_sequences=1)[0]\n",
    "corpus[0]['options'][2] = get_paraphrased_sentences(model, tokenizer, corpus[0]['options'][2], num_beams=2, num_return_sequences=1)[0]\n",
    "corpus[0]['options'][3] = get_paraphrased_sentences(model, tokenizer, corpus[0]['options'][3], num_beams=2, num_return_sequences=1)[0]\n",
    "corpus[0]['article'] = get_paraphrased_sentences(model, tokenizer, corpus[0]['article'], num_beams=2, num_return_sequences=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': 'B',\n",
       " 'options': [\"f : i understand . so you 'll be at the office and get nothing to do at 4 o'clock . let 's have a talk at that time .\",\n",
       "  \"f : okay , so you 'll be at the office at 4 o'clock . maybe we can have a talk at 6 o'clock .\",\n",
       "  \"f : great ! see you at the airport at 4 o'clock .\",\n",
       "  \"f : no problem ! see you at the restaurant at 4 o'clock . i ca n't wait to eat dinner with you since we 've finished a long talk .\"],\n",
       " 'article': \"f : hi , victor . can we have a talk today ? m : i 'd love to , but i 've got a pretty tight schedule today . i 'll finish a report by 10:00 and then drive to the airport to pick up an engineer at 11:00. after that , i 'll have a meeting with him over lunch . i wo n't have a break until 2 o'clock . but then from 3:00 until 5:00 , i have to attend a senior staff meeting .\",\n",
       " 'id': 'dev_792'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_dev[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': 'B',\n",
       " 'options': [\"I understand that you will be at the office at 4 o'clock and not have anything to do.\",\n",
       "  \"We can have a talk at 6 o'clock if you are at the office at 4 o'clock.\",\n",
       "  \"At 4 o'clock, see you at the airport.\",\n",
       "  \"I'm going to eat dinner with you at 4 o'clock.\"],\n",
       " 'article': \"Can we have a talk today, victor? I'll finish my report by 10:00 and drive to the airport to pick up an engineer.\",\n",
       " 'id': 'dev_792'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Define the output directory where you want to save the .txt files\n",
    "output_directory = \"Data/dev_para/\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "for index, json_data in enumerate(corpus):\n",
    "#for index, json_data in enumerate(new_corpus):\n",
    "    # Create a unique filename for each item in the corpus (you can change this as needed)\n",
    "    filename = f\"augmented_{index}.txt\"\n",
    "    \n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(output_directory, filename)\n",
    "    \n",
    "    # Convert the JSON data back to a string\n",
    "    json_str = json.dumps(json_data, indent=4)  # You can adjust the indentation as needed\n",
    "    \n",
    "    # Write the JSON string to the file\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(json_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Get the name of the first model\n",
    "first_model_name = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "\n",
    "# Get the tokenizer\n",
    "first_model_tkn = MarianTokenizer.from_pretrained(first_model_name)\n",
    "\n",
    "# Load the pretrained model based on the name\n",
    "first_model = MarianMTModel.from_pretrained(first_model_name)\n",
    "\n",
    "# Get the name of the second model\n",
    "second_model_name = 'Helsinki-NLP/opus-mt-fr-en'\n",
    "\n",
    "# Get the tokenizer\n",
    "second_model_tkn = MarianTokenizer.from_pretrained(second_model_name)\n",
    "\n",
    "# Load the pretrained model based on the name\n",
    "second_model = MarianMTModel.from_pretrained(second_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_batch_texts(language_code, batch_texts):\n",
    "  \n",
    "  formated_bach = [\">>{}<< {}\".format(language_code, text) for text in batch_texts]\n",
    "\n",
    "  return formated_bach\n",
    "\n",
    "def perform_translation(batch_texts, model, tokenizer, language=\"fr\"):\n",
    "  # Prepare the text data into appropriate format for the model\n",
    "  formated_batch_texts = format_batch_texts(language, batch_texts)\n",
    "  \n",
    "  # Generate translation using model\n",
    "  translated = model.generate(**tokenizer(formated_batch_texts, return_tensors=\"pt\", padding=True))\n",
    "\n",
    "  # Convert the generated tokens indices back into text\n",
    "  translated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "  \n",
    "  return translated_texts\n",
    "\n",
    "def perform_back_translation(batch_texts, original_language=\"en\", temporary_language=\"fr\"):\n",
    "\n",
    "  # Translate from Original to Tempora ry Language\n",
    "  tmp_translated_batch = perform_translation(batch_texts, first_model, first_model_tkn, temporary_language)\n",
    "\n",
    "  # Translate Back to English\n",
    "  back_translated_batch = perform_translation(tmp_translated_batch, second_model, second_model_tkn, original_language)\n",
    "\n",
    "  # Return The Final Result\n",
    "  return back_translated_batch\n",
    "\n",
    "def combine_texts(original_texts, back_translated_batch):\n",
    "  \n",
    "  return set(original_texts + back_translated_batch) \n",
    "\n",
    "def perform_back_translation_with_augmentation(batch_texts, original_language=\"en\", temporary_language=\"fr\"):\n",
    "\n",
    "  # Translate from Original to Temporary Language\n",
    "  tmp_translated_batch = perform_translation(batch_texts, first_model, first_model_tkn, temporary_language)\n",
    "\n",
    "  # Translate Back to English\n",
    "  back_translated_batch = perform_translation(tmp_translated_batch, second_model, second_model_tkn, original_language)\n",
    "\n",
    "  # Return The Final Result\n",
    "  # return combine_texts(original_texts, back_translated_batch)\n",
    "  return back_translated_batch\n",
    "\n",
    "\n",
    "\n",
    "#final_augmented = perform_back_translation_with_augmentation(original_texts)\n",
    "#final_augmented\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"m : have you seen my gloves anywhere ? i 've checked the cupboard but they 're not there . did i leave them on the desk ? f : oh yes , i remember i moved them from there and put them on the shelf by the window . i needed to do some work on the desk .\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "new_corpus = copy.deepcopy(corpus_dev[2])\n",
    "\n",
    "new_corpus['article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"F: I understand, so you'll be at the office and have nothing to do at 4 o'clock. Let's have a conversation at that time.\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perform_back_translation_with_augmentation({'I like turtles'})\n",
    "\n",
    "perform_back_translation_with_augmentation({corpus[0]['options'][0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "corpus = copy.deepcopy(corpus_dev)\n",
    "\n",
    "corpus[0]['options'][0] = perform_back_translation_with_augmentation({corpus[0]['options'][0]})\n",
    "corpus[0]['options'][1] = perform_back_translation_with_augmentation({corpus[0]['options'][1]})\n",
    "corpus[0]['options'][2] = perform_back_translation_with_augmentation({corpus[0]['options'][2]})\n",
    "corpus[0]['options'][3] = perform_back_translation_with_augmentation({corpus[0]['options'][3]})\n",
    "corpus[0]['article'] = perform_back_translation_with_augmentation({corpus[0]['article']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32my:\\UvA\\(DN) DL for NLP\\Project\\augmentations.ipynb Cell 16\u001b[0m line \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/y%3A/UvA/%28DN%29%20DL%20for%20NLP/Project/augmentations.ipynb#X26sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m corpus[\u001b[39m'\u001b[39m\u001b[39moptions\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m perform_back_translation_with_augmentation({corpus[\u001b[39m'\u001b[39m\u001b[39moptions\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m2\u001b[39m]})\n\u001b[0;32m      <a href='vscode-notebook-cell:/y%3A/UvA/%28DN%29%20DL%20for%20NLP/Project/augmentations.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m corpus[\u001b[39m'\u001b[39m\u001b[39moptions\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m3\u001b[39m] \u001b[39m=\u001b[39m perform_back_translation_with_augmentation({corpus[\u001b[39m'\u001b[39m\u001b[39moptions\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m3\u001b[39m]})\n\u001b[1;32m---> <a href='vscode-notebook-cell:/y%3A/UvA/%28DN%29%20DL%20for%20NLP/Project/augmentations.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m corpus[\u001b[39m'\u001b[39m\u001b[39marticle\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m perform_back_translation_with_augmentation({corpus[\u001b[39m'\u001b[39;49m\u001b[39marticle\u001b[39;49m\u001b[39m'\u001b[39;49m]})\n",
      "\u001b[1;32my:\\UvA\\(DN) DL for NLP\\Project\\augmentations.ipynb Cell 16\u001b[0m line \u001b[0;36mperform_back_translation_with_augmentation\u001b[1;34m(batch_texts, original_language, temporary_language)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/y%3A/UvA/%28DN%29%20DL%20for%20NLP/Project/augmentations.ipynb#X26sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m tmp_translated_batch \u001b[39m=\u001b[39m perform_translation(batch_texts, first_model, first_model_tkn, temporary_language)\n\u001b[0;32m     <a href='vscode-notebook-cell:/y%3A/UvA/%28DN%29%20DL%20for%20NLP/Project/augmentations.ipynb#X26sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# Translate Back to English\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/y%3A/UvA/%28DN%29%20DL%20for%20NLP/Project/augmentations.ipynb#X26sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m back_translated_batch \u001b[39m=\u001b[39m perform_translation(tmp_translated_batch, second_model, second_model_tkn, original_language)\n\u001b[0;32m     <a href='vscode-notebook-cell:/y%3A/UvA/%28DN%29%20DL%20for%20NLP/Project/augmentations.ipynb#X26sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# Return The Final Result\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/y%3A/UvA/%28DN%29%20DL%20for%20NLP/Project/augmentations.ipynb#X26sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# return combine_texts(original_texts, back_translated_batch)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/y%3A/UvA/%28DN%29%20DL%20for%20NLP/Project/augmentations.ipynb#X26sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mreturn\u001b[39;00m back_translated_batch\n",
      "\u001b[1;32my:\\UvA\\(DN) DL for NLP\\Project\\augmentations.ipynb Cell 16\u001b[0m line \u001b[0;36mperform_translation\u001b[1;34m(batch_texts, model, tokenizer, language)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/y%3A/UvA/%28DN%29%20DL%20for%20NLP/Project/augmentations.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m formated_batch_texts \u001b[39m=\u001b[39m format_batch_texts(language, batch_texts)\n\u001b[0;32m     <a href='vscode-notebook-cell:/y%3A/UvA/%28DN%29%20DL%20for%20NLP/Project/augmentations.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Generate translation using model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/y%3A/UvA/%28DN%29%20DL%20for%20NLP/Project/augmentations.ipynb#X26sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m translated \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtokenizer(formated_batch_texts, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/y%3A/UvA/%28DN%29%20DL%20for%20NLP/Project/augmentations.ipynb#X26sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Convert the generated tokens indices back into text\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/y%3A/UvA/%28DN%29%20DL%20for%20NLP/Project/augmentations.ipynb#X26sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m translated_texts \u001b[39m=\u001b[39m [tokenizer\u001b[39m.\u001b[39mdecode(t, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m translated]\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\torch\\autograd\\grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     27\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[1;32m---> 28\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\transformers\\generation\\utils.py:1685\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1678\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1679\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1680\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   1681\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1682\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1683\u001b[0m     )\n\u001b[0;32m   1684\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[1;32m-> 1685\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeam_search(\n\u001b[0;32m   1686\u001b[0m         input_ids,\n\u001b[0;32m   1687\u001b[0m         beam_scorer,\n\u001b[0;32m   1688\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m   1689\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1690\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[0;32m   1691\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[0;32m   1692\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[0;32m   1693\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1694\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[0;32m   1695\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1696\u001b[0m     )\n\u001b[0;32m   1698\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SAMPLE:\n\u001b[0;32m   1699\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[0;32m   1700\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\transformers\\generation\\utils.py:3024\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   3020\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   3022\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m-> 3024\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[0;32m   3025\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[0;32m   3026\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   3027\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   3028\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   3029\u001b[0m )\n\u001b[0;32m   3031\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   3032\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:1458\u001b[0m, in \u001b[0;36mMarianMTModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1453\u001b[0m     \u001b[39mif\u001b[39;00m decoder_input_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m decoder_inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1454\u001b[0m         decoder_input_ids \u001b[39m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   1455\u001b[0m             labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   1456\u001b[0m         )\n\u001b[1;32m-> 1458\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[0;32m   1459\u001b[0m     input_ids,\n\u001b[0;32m   1460\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1461\u001b[0m     decoder_input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[0;32m   1462\u001b[0m     encoder_outputs\u001b[39m=\u001b[39;49mencoder_outputs,\n\u001b[0;32m   1463\u001b[0m     decoder_attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[0;32m   1464\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1465\u001b[0m     decoder_head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[0;32m   1466\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[0;32m   1467\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1468\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1469\u001b[0m     decoder_inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[0;32m   1470\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1471\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1472\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1473\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1474\u001b[0m )\n\u001b[0;32m   1475\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(outputs[\u001b[39m0\u001b[39m]) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_logits_bias\n\u001b[0;32m   1477\u001b[0m masked_lm_loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:1259\u001b[0m, in \u001b[0;36mMarianModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1252\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1253\u001b[0m         last_hidden_state\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m0\u001b[39m],\n\u001b[0;32m   1254\u001b[0m         hidden_states\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1255\u001b[0m         attentions\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1256\u001b[0m     )\n\u001b[0;32m   1258\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1259\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[0;32m   1260\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[0;32m   1261\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[0;32m   1262\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_outputs[\u001b[39m0\u001b[39;49m],\n\u001b[0;32m   1263\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1264\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[0;32m   1265\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[0;32m   1266\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1267\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[0;32m   1268\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1269\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1270\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1271\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1272\u001b[0m )\n\u001b[0;32m   1274\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1275\u001b[0m     \u001b[39mreturn\u001b[39;00m decoder_outputs \u001b[39m+\u001b[39m encoder_outputs\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:1059\u001b[0m, in \u001b[0;36mMarianDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1048\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m   1049\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[0;32m   1050\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1056\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1057\u001b[0m     )\n\u001b[0;32m   1058\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1059\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[0;32m   1060\u001b[0m         hidden_states,\n\u001b[0;32m   1061\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1062\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1063\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m   1064\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49m(head_mask[idx] \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1065\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49m(\n\u001b[0;32m   1066\u001b[0m             cross_attn_head_mask[idx] \u001b[39mif\u001b[39;49;00m cross_attn_head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[0;32m   1067\u001b[0m         ),\n\u001b[0;32m   1068\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m   1069\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1070\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1071\u001b[0m     )\n\u001b[0;32m   1072\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1074\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:431\u001b[0m, in \u001b[0;36mMarianDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    429\u001b[0m self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[39m# add present self-attn cache to positions 1,2 of present_key_value tuple\u001b[39;00m\n\u001b[1;32m--> 431\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[0;32m    432\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[0;32m    433\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    434\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    435\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[0;32m    436\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    437\u001b[0m )\n\u001b[0;32m    438\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[0;32m    439\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:215\u001b[0m, in \u001b[0;36mMarianAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[39melif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[39m# reuse k, v, self_attention\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     key_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj(hidden_states), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, bsz)\n\u001b[1;32m--> 215\u001b[0m     value_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mv_proj(hidden_states), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, bsz)\n\u001b[0;32m    216\u001b[0m     key_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([past_key_value[\u001b[39m0\u001b[39m], key_states], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m    217\u001b[0m     value_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([past_key_value[\u001b[39m1\u001b[39m], value_states], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\Bunplaya\\anaconda3\\envs\\dl2021\\lib\\site-packages\\torch\\nn\\functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[0;32m   1847\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[1;32m-> 1848\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import copy\n",
    "new_corpus = copy.deepcopy(corpus_dev)\n",
    "\n",
    "\n",
    "for corpus in new_corpus:\n",
    "    corpus['options'][0] = perform_back_translation_with_augmentation({corpus['options'][0]})\n",
    "    corpus['options'][1] = perform_back_translation_with_augmentation({corpus['options'][1]})\n",
    "    corpus['options'][2] = perform_back_translation_with_augmentation({corpus['options'][2]})\n",
    "    corpus['options'][3] = perform_back_translation_with_augmentation({corpus['options'][3]})\n",
    "    corpus['article'] = perform_back_translation_with_augmentation({corpus['article']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': 'A',\n",
       " 'options': [\"f : yeah , you ca n't complain directly to the manager because he is on vacation . i am sorry that you do n't like the haircut .\",\n",
       "  \"f : yeah , you ca n't complain directly to the manager because he is on vacation . do n't worry , i did n't cut your cloth too short .\",\n",
       "  \"f : yeah , you ca n't complain directly to the manager because he is out to lunch . calm down , you are a teacher !\",\n",
       "  \"f : yeah , you ca n't complain directly to the manager because he is dealing with another customer . sorry about that .\"],\n",
       " 'article': \"m : hi , i 'd like to get my haircut . f : well , can we interest you in today 's special ? we 'll shampoo cut and style your hair for one unbelievable low price of $ 12.00 . m : ok , but i just want to get my haircut a little bit . a little off the top and sides . that 's all . f : no problem . ok , here we go . so what do you do for a living ? m : i 'm a lawyer and i 'm in town for a job interview and ... f : oops ! m : what do you mean '' oops '' ? hey , can i see a mirror ? f : nothing to worry about , sir . relax ! m : ouch ! that really hurt . what are you doing , anyway ? oh , look at all my hair on the floor . how much are you really cutting off ? f : relax . time for the shampoo . m : hey , you got shampoo in my eyes . where 's the towel ? oh , gosh , you cut my hair too short . i want to talk to the manager now . f : i 'm sorry , but he 's on vacation . m : uh .\",\n",
       " 'id': 'dev_454'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_dev[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': 'A',\n",
       " 'options': [[\"F: Yes, you don't complain directly to the manager because he's on vacation. I'm sorry you don't like the haircut.\"],\n",
       "  [\"F: Yes, you don't complain directly to the manager because he's on vacation.Don't worry, I didn't cut your fabric too short.\"],\n",
       "  [\"F: Yes, you don't complain directly to the manager because he's out for lunch. Calm down, you're a teacher!\"],\n",
       "  [\"F: Yes, you don't complain directly to the manager because he's dealing with another client. Sorry about that.\"]],\n",
       " 'article': \"m : hi , i 'd like to get my haircut . f : well , can we interest you in today 's special ? we 'll shampoo cut and style your hair for one unbelievable low price of $ 12.00 . m : ok , but i just want to get my haircut a little bit . a little off the top and sides . that 's all . f : no problem . ok , here we go . so what do you do for a living ? m : i 'm a lawyer and i 'm in town for a job interview and ... f : oops ! m : what do you mean '' oops '' ? hey , can i see a mirror ? f : nothing to worry about , sir . relax ! m : ouch ! that really hurt . what are you doing , anyway ? oh , look at all my hair on the floor . how much are you really cutting off ? f : relax . time for the shampoo . m : hey , you got shampoo in my eyes . where 's the towel ? oh , gosh , you cut my hair too short . i want to talk to the manager now . f : i 'm sorry , but he 's on vacation . m : uh .\",\n",
       " 'id': 'dev_454'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_corpus[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synonym Replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Bunplaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "\n",
    "# Download WordNet data (run once)\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.append(lemma.name())\n",
    "    return list(set(synonyms))\n",
    "\n",
    "def synonym_replacement(text, n=1):\n",
    "    words = text.split()\n",
    "    augmented_text = []\n",
    "\n",
    "    for word in words:\n",
    "        if wordnet.synsets(word):\n",
    "            synonyms = get_synonyms(word)\n",
    "            if synonyms:\n",
    "                random.shuffle(synonyms)\n",
    "                synonym = synonyms[0]\n",
    "                augmented_text.append(synonym)\n",
    "            else:\n",
    "                augmented_text.append(word)\n",
    "        else:\n",
    "            augmented_text.append(word)\n",
    "\n",
    "    return ' '.join(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"f : single realise . hence you 'll represent at the berth and stimulate nada to set astatine quaternion o'clock . allow 's take angstrom verbalise atomic_number_85 that clock_time .\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "corpus = copy.deepcopy(corpus_dev)\n",
    "\n",
    "synonym_replacement(corpus[0]['options'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "new_corpus = copy.deepcopy(corpus_dev)\n",
    "\n",
    "for corpus in new_corpus:\n",
    "    corpus['options'][0] = synonym_replacement(corpus['options'][0])\n",
    "    corpus['options'][1] = synonym_replacement(corpus['options'][1])\n",
    "    corpus['options'][2] = synonym_replacement(corpus['options'][2])\n",
    "    corpus['options'][3] = synonym_replacement(corpus['options'][3])\n",
    "    corpus['article'] = synonym_replacement(corpus['article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': 'B',\n",
       " 'options': [\"f : i understand . so you 'll be at the office and get nothing to do at 4 o'clock . let 's have a talk at that time .\",\n",
       "  \"f : okay , so you 'll be at the office at 4 o'clock . maybe we can have a talk at 6 o'clock .\",\n",
       "  \"f : great ! see you at the airport at 4 o'clock .\",\n",
       "  \"f : no problem ! see you at the restaurant at 4 o'clock . i ca n't wait to eat dinner with you since we 've finished a long talk .\"],\n",
       " 'article': \"f : hi , victor . can we have a talk today ? m : i 'd love to , but i 've got a pretty tight schedule today . i 'll finish a report by 10:00 and then drive to the airport to pick up an engineer at 11:00. after that , i 'll have a meeting with him over lunch . i wo n't have a break until 2 o'clock . but then from 3:00 until 5:00 , i have to attend a senior staff meeting .\",\n",
       " 'id': 'dev_792'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_dev[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': 'B',\n",
       " 'options': [\"fluorine : I translate . indeed you 'll represent at the office_staff and perplex cipher to fare atomic_number_85 four o'clock . let 's suffer amp speak astatine that clock_time .\",\n",
       "  \"atomic_number_9 : OK , sol you 'll exist at the authority At quatern o'clock . perhaps we dismiss cause angstrom talk_of_the_town astatine sise o'clock .\",\n",
       "  \"fluorine : bang-up ! figure you At the aerodrome atomic_number_85 quaternion o'clock .\",\n",
       "  \"farad : nobelium job ! date you At the restaurant at iv o'clock . iodine Golden_State n't hold to eat_on dinner_party with you since we 've finished angstrom_unit farseeing talk_of_the_town .\"],\n",
       " 'article': \"farad : Hawai'i , victor . posterior we own ampere speak today ? molarity : iodine 'd know to , simply ace 've puzzle angstrom_unit somewhat slopped agenda today . iodin 'll land_up group_A written_report past 10:00 and so crusade to the airport to plunk upward AN direct atomic_number_85 11:00. later that , ace 'll ingest axerophthol fulfil with him complete tiffin . 1 wo n't bear a stop until two o'clock . merely then from 3:00 until 5:00 , single hold to see amp older faculty receive .\",\n",
       " 'id': 'dev_792'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Define the output directory where you want to save the .txt files\n",
    "output_directory = \"Data/dev_syn/\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "for index, json_data in enumerate(corpus):\n",
    "#for index, json_data in enumerate(new_corpus):\n",
    "    # Create a unique filename for each item in the corpus (you can change this as needed)\n",
    "    filename = f\"augmented_{index}.txt\"\n",
    "    \n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(output_directory, filename)\n",
    "    \n",
    "    # Convert the JSON data back to a string\n",
    "    json_str = json.dumps(json_data, indent=4)  # You can adjust the indentation as needed\n",
    "    \n",
    "    # Write the JSON string to the file\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(json_str)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
